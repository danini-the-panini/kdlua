describe("tokenizer", function()
  local tokenizer = require "kdl.tokenizer"

  it("can peek at upcoming tokens", function()
    local t = tokenizer.new("node 1 2 3")
    assert.same(t:peek(), { type="IDENT", value="node" })
    assert.same(t:peek_next(), { type="WS", value=" " })
    assert.same(t:next(), { type="IDENT", value="node" })
    assert.same(t:peek(), { type="WS", value=" " })
    assert.same(t:peek_next(), { type="INTEGER", value=1 })
  end)

  it("tokenizes identifiers", function()
    assert.same(tokenizer.new("foo"):next(), { type="IDENT", value="foo" })
  end)

  it("tokenizes strings", function()
    assert.same(tokenizer.new('"foo"'):next(), { type="STRING", value="foo" })
    assert.same(tokenizer.new('"foo\\nbar"'):next(), { type="STRING", value="foo\nbar" })
    assert.same(tokenizer.new('"\\u{10FFF}"'):next(), { type="STRING", value="\u{10FFF}" })
  end)

  it("tokenizes rawstrings", function()
    assert.same(tokenizer.new('#"foo\\nbar"#'):next(), { type="RAWSTRING", value="foo\\nbar" })
    assert.same(tokenizer.new('#"foo"bar"#'):next(), { type="RAWSTRING", value="foo\"bar" })
    assert.same(tokenizer.new('##"foo"#bar"##'):next(), { type="RAWSTRING", value="foo\"#bar" })
    assert.same(tokenizer.new('#""foo""#'):next(), { type="RAWSTRING", value="\"foo\"" })

    local t = tokenizer.new('node #"C:\\Users\\zkat\\"#')
    assert.same(t:next(), { type="IDENT", value="node" })
    assert.same(t:next(), { type="WS", value=" " })
    assert.same(t:next(), { type="RAWSTRING", value="C:\\Users\\zkat\\" })

    t = tokenizer.new('other-node #"hello"world"#')
    assert.same(t:next(), { type="IDENT", value="other-node" })
    assert.same(t:next(), { type="WS", value=" " })
    assert.same(t:next(), { type="RAWSTRING", value="hello\"world" })
  end)

  it("tokenizes integers", function()
    assert.same(tokenizer.new("0x0123456789abcdef"):next(), { type="INTEGER", value=0x0123456789abcdef })
    assert.same(tokenizer.new("0o01234567"):next(), { type="INTEGER", value=342391 })
    assert.same(tokenizer.new("0b101001"):next(), { type="INTEGER", value=41 })
    assert.same(tokenizer.new("-0x0123456789abcdef"):next(), { type="INTEGER", value=-0x0123456789abcdef })
    assert.same(tokenizer.new("-0o01234567"):next(), { type="INTEGER", value=-342391 })
    assert.same(tokenizer.new("-0b101001"):next(), { type="INTEGER", value=-41 })
    assert.same(tokenizer.new("+0x0123456789abcdef"):next(), { type="INTEGER", value=0x0123456789abcdef })
    assert.same(tokenizer.new("+0o01234567"):next(), { type="INTEGER", value=342391 })
    assert.same(tokenizer.new("+0b101001"):next(), { type="INTEGER", value=41 })
  end)

  it("tokenizes floats", function()
    assert.same(tokenizer.new("1.23"):next(), { type="FLOAT", value=1.23 })
    assert.same(tokenizer.new("#inf"):next(), { type="FLOAT", value=math.huge })
    assert.same(tokenizer.new("#-inf"):next(), { type="FLOAT", value=-math.huge })
    local nan = tokenizer.new("#nan"):next()
    assert.same(nan.type, "FLOAT")
    assert.is_not.equal(nan.value, nan.value);
  end)

  it("tokenizers booleans", function()
    assert.same(tokenizer.new("#true"):next(), { type="TRUE", value=true })
    assert.same(tokenizer.new("#false"):next(), { type="FALSE", value=false })
  end)

  it("tokenizers nulls", function()
    assert.same(tokenizer.new("#null"):next(), { type="NULL", value=nil })
  end)

  it("tokenizers symbols", function()
    assert.same(tokenizer.new("{"):next(), { type="LBRACE", value="{" })
    assert.same(tokenizer.new("}"):next(), { type="RBRACE", value="}" })
  end)

  it("tokenizes equals", function()
    assert.same(tokenizer.new("="):next(), { type="EQUALS", value="=" })
    assert.same(tokenizer.new(" ="):next(), { type="EQUALS", value=" =" })
    assert.same(tokenizer.new("= "):next(), { type="EQUALS", value="= " })
    assert.same(tokenizer.new(" = "):next(), { type="EQUALS", value=" = " })
    assert.same(tokenizer.new(" =foo"):next(), { type="EQUALS", value=" =" })
    assert.same(tokenizer.new("\u{FE66}"):next(), { type="EQUALS", value="\u{FE66}" })
    assert.same(tokenizer.new("\u{FF1D}"):next(), { type="EQUALS", value="\u{FF1D}" })
    assert.same(tokenizer.new("üü∞"):next(), { type="EQUALS", value="üü∞" })
  end)

  it("tokenizes whitespace", function()
    assert.same(tokenizer.new(" "):next(), { type="WS", value=" " })
    assert.same(tokenizer.new("\t"):next(), { type="WS", value="\t" })
    assert.same(tokenizer.new("    \t"):next(), { type="WS", value="    \t" })
    assert.same(tokenizer.new("\\\n"):next(), { type="WS", value="\\\n" })
    assert.same(tokenizer.new("\\"):next(), { type="WS", value="\\" })
    assert.same(tokenizer.new("\\//some comment\n"):next(), { type="WS", value="\\\n" })
    assert.same(tokenizer.new("\\ //some comment\n"):next(), { type="WS", value="\\ \n" })
    assert.same(tokenizer.new("\\//some comment"):next(), { type="WS", value="\\" })
    assert.same(tokenizer.new(" \\\n"):next(), { type="WS", value=" \\\n" })
    assert.same(tokenizer.new(" \\//some comment\n"):next(), { type="WS", value=" \\\n" })
    assert.same(tokenizer.new(" \\ //some comment\n"):next(), { type="WS", value=" \\ \n" })
    assert.same(tokenizer.new(" \\//some comment"):next(), { type="WS", value=" \\" })
    assert.same(tokenizer.new(" \\\n  \\\n  "):next(), { type="WS", value=" \\\n  \\\n  " })
  end)

  it("tokenizes multiple tokens", function()
    local t = tokenizer.new("node 1 \"two\" a=3")

    assert.same(t:next(), { type="IDENT", value="node" })
    assert.same(t:next(), { type="WS", value=" " })
    assert.same(t:next(), { type="INTEGER", value=1 })
    assert.same(t:next(), { type="WS", value=" " })
    assert.same(t:next(), { type="STRING", value="two" })
    assert.same(t:next(), { type="WS", value=" " })
    assert.same(t:next(), { type="IDENT", value="a" })
    assert.same(t:next(), { type="EQUALS", value="=" })
    assert.same(t:next(), { type="INTEGER", value=3 })
    assert.same(t:next(), { type="EOF", value="" })
    assert.same(t:next(), { type=false, value=false })
  end)

  it("tokenizes single line comments", function()
    assert.same(tokenizer.new("// comment"):next(), { type="EOF", value="" })

    local t = tokenizer.new([[node1
// comment
node2]])

    assert.same(t:next(), { type="IDENT", value="node1" })
    assert.same(t:next(), { type="NEWLINE", value="\n" })
    assert.same(t:next(), { type="NEWLINE", value="\n" })
    assert.same(t:next(), { type="IDENT", value="node2" })
    assert.same(t:next(), { type="EOF", value="" })
    assert.same(t:next(), { type=false, value=false })
  end)

  it("tokenizes multiline comments", function()
    local t = tokenizer.new("foo /*bar=1*/ baz=2")

    assert.same(t:next(), { type="IDENT", value="foo" })
    assert.same(t:next(), { type="WS", value="  " })
    assert.same(t:next(), { type="IDENT", value="baz" })
    assert.same(t:next(), { type="EQUALS", value="=" })
    assert.same(t:next(), { type="INTEGER", value=2 })
    assert.same(t:next(), { type="EOF", value="" })
    assert.same(t:next(), { type=false, value=false })
  end)

  it("tokenizes utf8", function()
    assert.same(tokenizer.new("üòÅ"):next(), { type="IDENT", value="üòÅ" })
    assert.same(tokenizer.new('"üòÅ"'):next(), { type="STRING", value="üòÅ" })
    assert.same(tokenizer.new("„Éé„Éº„Éâ"):next(), { type="IDENT", value="„Éé„Éº„Éâ" })
    assert.same(tokenizer.new("„ÅäÂêçÂâç"):next(), { type="IDENT", value="„ÅäÂêçÂâç" })
    assert.same(tokenizer.new('"‚òú(Ôæü„ÉÆÔæü‚òú)"'):next(), { type="STRING", value="‚òú(Ôæü„ÉÆÔæü‚òú)" })

    local t = tokenizer.new([[smile "üòÅ"
„Éé„Éº„Éâ „ÅäÂêçÂâçÔºù"‚òú(Ôæü„ÉÆÔæü‚òú)"]])

    assert.same(t:next(), { type="IDENT", value="smile" })
    assert.same(t:next(), { type="WS", value=" " })
    assert.same(t:next(), { type="STRING", value="üòÅ" })
    assert.same(t:next(), { type="NEWLINE", value="\n" })
    assert.same(t:next(), { type="IDENT", value="„Éé„Éº„Éâ" })
    assert.same(t:next(), { type="WS", value=" " })
    assert.same(t:next(), { type="IDENT", value="„ÅäÂêçÂâç" })
    assert.same(t:next(), { type="EQUALS", value="Ôºù" })
    assert.same(t:next(), { type="STRING", value="‚òú(Ôæü„ÉÆÔæü‚òú)" })
    assert.same(t:next(), { type="EOF", value="" })
    assert.same(t:next(), { type=false, value=false })
  end)

  it("tokenizes semicolons", function()
    local t = tokenizer.new("node1; node2")

    assert.same(t:next(), { type="IDENT", value="node1" })
    assert.same(t:next(), { type="SEMICOLON", value=";" })
    assert.same(t:next(), { type="WS", value=" " })
    assert.same(t:next(), { type="IDENT", value="node2" })
    assert.same(t:next(), { type="EOF", value="" })
    assert.same(t:next(), { type=false, value=false })
  end)

  it("tokenizes slash dash", function()
    local t = tokenizer.new([[/-mynode /-"foo" /-key=1 /-{
  a
}]])

    assert.same(t:next(), { type="SLASHDASH", value="/-" })
    assert.same(t:next(), { type="IDENT", value="mynode" })
    assert.same(t:next(), { type="WS", value=" " })
    assert.same(t:next(), { type="SLASHDASH", value="/-" })
    assert.same(t:next(), { type="STRING", value="foo" })
    assert.same(t:next(), { type="WS", value=" " })
    assert.same(t:next(), { type="SLASHDASH", value="/-" })
    assert.same(t:next(), { type="IDENT", value="key" })
    assert.same(t:next(), { type="EQUALS", value="=" })
    assert.same(t:next(), { type="INTEGER", value=1 })
    assert.same(t:next(), { type="WS", value=" " })
    assert.same(t:next(), { type="SLASHDASH", value="/-" })
    assert.same(t:next(), { type="LBRACE", value="{" })
    assert.same(t:next(), { type="NEWLINE", value="\n" })
    assert.same(t:next(), { type="WS", value="  " })
    assert.same(t:next(), { type="IDENT", value="a" })
    assert.same(t:next(), { type="NEWLINE", value="\n" })
    assert.same(t:next(), { type="RBRACE", value="}" })
    assert.same(t:next(), { type="EOF", value="" })
    assert.same(t:next(), { type=false, value=false })
  end)

  it("tokenizes multiline nodes", function()
    local t = tokenizer.new([[title \
  "Some title"]])

    assert.same(t:next(), { type="IDENT", value="title" })
    assert.same(t:next(), { type="WS", value=" \\\n  " })
    assert.same(t:next(), { type="STRING", value="Some title" })
    assert.same(t:next(), { type="EOF", value="" })
    assert.same(t:next(), { type=false, value=false })
  end)

  it("tokenizes types", function()
    local t = tokenizer.new("(foo)bar")
    assert.same(t:next(), { type="LPAREN", value="(" })
    assert.same(t:next(), { type="IDENT", value="foo" })
    assert.same(t:next(), { type="RPAREN", value=")" })
    assert.same(t:next(), { type="IDENT", value="bar" })

    t = tokenizer.new("(foo)/*asdf*/bar")
    assert.same(t:next(), { type="LPAREN", value="(" })
    assert.same(t:next(), { type="IDENT", value="foo" })
    assert.same(t:next(), { type="RPAREN", value=")" })
    assert.same(t:next(), { type="IDENT", value="bar" })

    t = tokenizer.new("(foo/*asdf*/)bar")
    assert.same(t:next(), { type="LPAREN", value="(" })
    assert.same(t:next(), { type="IDENT", value="foo" })
    assert.same(t:next(), { type="RPAREN", value=")" })
    assert.same(t:next(), { type="IDENT", value="bar" })
  end)
end)